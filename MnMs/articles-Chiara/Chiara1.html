<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>A robust music genre classification approach for global and regional music datasets evaluation</title>
</head>

<body>
	<header>
		<h1>A robust music genre classification approach for global and regional music datasets evaluation</h1>
		<p>Jefferson Martins de Sousa; Eanes Torres Pereira; Luciana Ribeiro Veloso<!--decidiamo un modo unico in cui scrivere gli autori--></p>
		<!--<p>Date: ?</p>-->
	</header>

	<article>
		<section>
			<h2><center>I. Introduction</center></h2>
			<p>The 21st century began and so the number of streaming music, empowered by the use of internet in all the world. With the huge availability of digital music, there is a need to organize them. Genres are a popular way to organize large music collections, both private and commercial. In the context of genre music classification, many approaches have been proposed such as: melody [1], source separation [2] and different aspects of music features (low and high-level features).</p>

			<p>Currently, musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to Music Information Retrieval (MIR) systems.</p>

			<p>There is a considerable amount of proposed approaches on extracting descriptive features for music genre classification [3]. In this paper, we propose a set of features to classify genres of music based on a methodical selection of important features to music information retrieval (MIR) and music emotion recognition (MER).</p>

			<p>Various methods for feature extraction have been developed. Sturm [4], compared MGR (Music Genre Recognition) works which used the GTZAN dataset. According to his paper, we selected all the approaches which performed experiments with 5-fold cross validation and achieved the best accuracies. Below, we describe a review of these papers.</p>

			<p>Zeng et al. [5] proposed extract features decomposing music signal into a bag of audio words, using latent semantic analysis model (pLSA) for classification. pLSA is a model proposed by Hofmann [6]. Zeng et al. [5] extracted the first 20 MFCC coefficients and its first-order derivative, and achieved 81.5% of accuracy.</p>

			<p>Manzagol et al. [7] comes with a new proposal using sparse coding and AdaBoost for classifying audio streams. With the approach used, they achieved 54.3% of precision. Holzapfel et al. [8] used a non-negative matrix factorization (NMF) technique in order to derive a novel description for the timbre of musical sounds. Their approach improved classification compared to methods using MFCC and achieved 74% of accuracy. Holzapfel et al. [9] present an approach using a new set of features based on Non-negative Matrix Factorization for classification of musical signals into genres. Their results show a superiority of the proposed features compared to MFCC. They achieved 74.5% of accuracy. Gaussian Mixture Model (GMM) is used instead of SVM to create a model of genres. According to Sturm [4], these are the last articles published using GTZAN and 5-fold cross-validation.</p>

			<p>There are two main approaches to classify emotions in music: categorical and dimensional [10]. Panda et al. [11] use a categorical approach to classify mood in music using three different frameworks to extract features: Marsyas [12], MIR Toolbox [13] and PsySound3 [14]. They use a SVM model trained to differentiate between the five existing clusters. The approach proposed by Panda won the MIREX contest of 2012 [11] with 67.83% of accuracy.</p>

			<p>Yang et al. [15] used a dimensional approach to classify mood in music by extracting features using PsySound [14], Marsyas [12] and two DWCH algorithms [16]. These features were given as input to a SVM regression method for predicting arousal and valence. Coefficient of determination (R2) was computed to evaluate their method on a dataset proposed in the paper. Their method achieved 58.3% of R2 (coefficient of determination) for arousal and 28% for valence.</p>

			<p>We evaluated our method using the GTZAN [17] dataset which is used for other researchers and applied our approach to creating a model to classify genres from <span class="placeName">Brazil</span>.<p>

			<p>Although music genres do not have strict definitions [17], music genre annotation is performed manually in many websites. However, manual annotation of genres, takes much time and genres can be quite subjective. Due to that, this paper proposes an automatic musical genre classification using six sets of descriptors: spectral, time-domain, tonal, rhythm, sound effect, and high-level. Besides, we propose a music dataset with popular genres in <span class="placeName">Brazil</span>.</p>

			<p>This paper is organized as follows. In section 2 we describe the proposed method. In section 3 we describe the datasets used for evaluation and present the classification results obtained. Conclusions of the work are provided in section 4.</p>
		</section>
		
		<section>
			<h2><center>II. Method</center></h2>
			<p>We employed a four-step method for genre classification: dataset creation, feature extraction, classification and evaluation. Although there are many and different kinds of music genre datasets, there is not any dataset with all important genres of the northeast area of Brazil. We found the Audio Latin Genre Classification Dataset2 which includes some important genres of Brazil as forró, pagode, sertanejo and axé, but still lacks some important genres from the northeast area of Brazil, as Repente and a characteristic genre, very similar to the MPB (brazilian popular music) music. Owing to the problem of lack of genres, we proposed the creation of a new dataset that includes others important genres of Brazil withdrawn from the northeast area. Another reason to create the brazilian music dataset is because we did not find any classifier to the genres we call MPB, Repente and Brega. The genre we call MPB is widely produced by many singers from the northeast and it is important to have a classifier to this important genre. The same we say to Repente, which is widely spread to all the northeast region and until today there is not any music dataset containing samples labeled with Repente.</p>

			<p>The Brazilian music dataset was constructed with 7 musical genres (Forró, Rock, Repente, MPB, Brega, Sertanejo and Disco) in which the representative part of genre was selected from the whole music. These genres were chosen because they are typical genres of the brazilian music and until now, there is no other classifier implemented to these genres. Each genre has 30 songs with a frequency of 44100Hz sampling rate. We selected excerpts from a wide set of artists to cover a large variation of music in a genre.</p>

			<p>We searched for video-clips in Youtube and mp3 songs in PaicoMP33 related to the seven categories of genres chosen for composing our proposed dataset. For each genre class we selected 30 audio tracks and used Audacity4 , a software for recording and editing sounds, to select and crop the music part that best represents the genre.</p>

			<p>In order to create a set of features to best classify music genres, we made a methodical selection in papers of music information retrieval (MIR) and music emotion recognition (MER), which have achieved the best accuracies. We wanted to find what were the best features used in both types of system and how we could mix these features in order to create the best set of features to MGR. We searched for papers of MIR and MER MER, we selected papers with less than or 5 years of publication.<p>

			<p>At the end of our search we got 30 articles to select what were the best features we could use to combine and create a new set of features proposed in this paper. We noticed that many articles used repeated features and were only using different classification methods or datasets. With this in mind we reject most part of these 30 papers and stayed with 6 papers that presented higher accuracies among all the papers. The papers selected were: Zeng et al. [5], Manzagol et al. [7], Holzapfel et al. [8], Holzapfel et al. [9], Panda et al. [11], Yang et al. [15]. The criteria to select which features we would use to compose our set of features were: popularity of the feature (if the feature were used in different papers) and features that were presented as important to MIR.</p>

			<p>One of the challenges of working with pattern recognition is to find the best features to solve problems in MIR and the best way to extract them, besides that many features are usually hard to extract from audio signals (high-level features: Danceability, Dynamic Complexity, etc). Although several authors have studied the most relevant musical attributes for genre classification, there is not a fixed set of features for the music genre classification problem [18], [19]. Considering this, we used features which belong to 6 categories of descriptors: spectral, time domain, rhythm, sound effect, and high-level. Besides the common features presented in most papers, we also extracted the following features: loudness, sharpness, dissonance, tonality, tempo, inharmonicity, key and beat histograms.</p>

			<p>In order to represent the set of feature extracted for each song, we chose a single vector among three forms of feature representation. To take the average or median values for each feature attribute over all segments is a traditional strategy used [20]. We took a different strategy. For the single vector representation, a set of statistics was computed to summarize the frame descriptor extracted for each song. For each feature attribute we took the mean, mean of the derivative, mean of the second derivative, variance, variance of the derivative, variance of the second derivative, minimum and maximum.</p>

			<p>There are three common choices of classifiers used for music information retrieval: k-Nearest Neighbors (K-NN), Support Vector Machine (SVM) and Gaussian Mixture Model (GMM). Support Vector Machines are among the top performing classifiers [20]. Therefore we opted to use the SVM in our experiments.</p>

			<p>The feature extraction algorithm was coded in C++, using Essentia5 library to extract relevant audio features and manipulate data. In our experiments, the LibSym6 implementation of SVM was applied.</p>
		</section>
		
		<section>
			<h2><center>III. Evaluation</center></h2>
			<p>We evaluated our approaches by classification accuracy computed from k-fold cross-validation and splitting our data in training and test. Both methods to measure the approaches are very used in the literature [3]. Following papers which split the dataset 70/30 (70% used to train, 30% used to test) are: [21] and [22]. Following papers which used 5-fold cross-validation are: [5], [7]–​[9].</p>

			<p>We randomly sorted the GTZAN dataset with 1000 pieces and selected the first 667 to train and the latest 333 to test the model created. Representing two thirds (66.7%) of the dataset to train and one third (33.3%) to test. This process was repeated 30 times. We opted to repeat 30 times, because we wanted to show the result found is not a consequence of a random grouping of the best group of songs used to created the model. Therefore, our goal is to present the robustness of our method, repeating 30 times with random sets of songs used to test and train. In the end, we had 9990 pieces of music which were tested. The result is presented in Table I. The average accuracy of this method was 78,00%. Table II shows the result of one repetition of those 30 repetitions shown in Table I. For one repetition the average accuracy was 81.35%.</p>

			<p>The accuracy to rock class was less than 50%. The problem is caused due to the rhythm and the beat of rock songs be very similar to metal and country. This is why we see our model predicting country and metal when it should predict rock. We see the same pattern when we check the Table I and we see that the model predicted rock when it should predict metal and country songs.</p>

			<p>Among the genres presented in GTZAN dataset, the genres classical and jazz are the ones which has most differences in rhythm and beat compared to the others genres. However, the model did not have much difficulty to differentiate classical and jazz music and achieved the best accuracies. As we check in Table I.</p>

			<p>We ran 5-fold cross-validation dividing each genre of the data set in 5 folds. We had the accuracy of 79.70% for this method. There are some genres that had accuracy lower than 70%, Rock with 49.31%, Disco with 65.87% and Country with 69.68%. Reggae had accuracy of 75.37% and all the others genre had accuracy above 80%.</p>
			
			<p>For the brazilian music dataset we ran 5-fold cross-validation and got the rate of 86.11%. From one of the 30 times we ran 5-fold cross validation, with the best set of audios for training and testing, our model achieved 97.62% of accuracy. Two genres that had the lowest rates were Forro and Rock.</p>

			<p>Table III presents the results of works that measured their performance by classification accuracy computed from 5-fold cross-validation [3]. Zeng et al. [5] achieved 1.8% of accuracy higher than results obtained by our proposed approach. This result may be because they performed 5-fold cross validation only one time and used the best set of audios to train and test the model created. We performed 30 runs of 5-fold cross validation and used the average of all results to represent our accuracy, hence we can ensure our model is good not only when we used the best set of features to train and test.
			Table I Confusion matrix showing the results of 30 repetition of the gtzan dataset divided in 66.7% to train and 33.3% to test. The cell values correspond to the accumulation of classification results for 30 repetition
			Table II Confusion matrix presenting the results of one repetition of the gtzan dataset divided in 66.7% to train and 33.3% to test.
			Table III Best accuracies presented in previous works using GTZAN.</p>

			<p>According to Table I and Table II, we noticed how good our approach performed with the set of features proposed. Many authors, when performing n-fold cross validation, perform only a single run [22], [23]. That is not a good way of verifying the generality of the method. When they use only 10%, in a 10-fold cross-validation procedure, with randomly shuffling the samples and performing many executions, they may have used the best set of samples to test their model and consequently they have achieved great accuracies. This is why we repeated the process of train and test 30 times with the GTZAN dataset, hence we could have the idea of how good or bad our approach is.</p>

			<p>Comparing our result with the latest results of MIREX and ISMIR we noticed that our approach achieved similar accuracy results. Running our approach on GTZAN data set and using 5-fold cross-validation to evaluate, our average result, after 30 executions of cross-validation, was 79.70%.</p>

			<p>According to the results using GTZAN presented in Table I and Table II and the brazilian music dataset, we notice an improvement in MGR using the set of features proposed in this article. Since GTZAN (the dataset we used) and the genre classification dataset (dataset used in MIREX) have 10 genres and both datasets use blues, classical, country, hip hop, metal, jazz, disco and rock, summing 8 equal genres in both datasets. Hence with 8 genres in common, the GTZAN dataset are very similar to the dataset used in MIREX. Our accuracy of 79.7% is still better than accuracies presented in MIREX in 2014 and 2015 (69% to 76%).</p>

			<p>Based on the review of state of art in MGR, using GTZAN and 5-fold cross-validation published by Bob L. Sturm in 2013 [3], our result is competitive to theirs. The accuracies presented on the review varies between 54.3% and 81.5%. We also conclude that the set of features we propose has an important relevance to MGR, due to the high accuracies we achieved not only with the GTZAN dataset but also with the dataset we created.</p>
		</section>
		
		<section>
			<h2><center>IV. Conclusion</center></h2>
			<p>A good set of features is important to achieve high accuracies [3]. Unfortunately, until today many researchers use different features to predict genres in music. The fact that the researches don't have a fixed set of features to start their works turns the research slowly [4], and the researcher needs to select features, a dataset and a method of classification. If the chosen features are not relevant to MGR, even with a good method of classification the research may achieve low accuracies. According to our evaluation we concluded that the set of features proposed in this article is relevant to MGR. Furthermore our results is equivalent to current state of art in MGR [3].</p>

			<p>The latin music database [24] and the MIREX 2009 genre latin classification dataset presents some genres that we also included in BMD. But because these two datasets are focused on music produced in all countries of the latin America, they still lack some important features that represent the brazilian music. In our dataset we included Repente and Brega that are two important genres to the brazilian music. The creation of the brazilian music dataset is important because there are a great amount of music being produced in this region of Brazil and there is not an automatic method to classify some of these genres (Repente MPB and Brega). As presented in the evaluation section, we used the set of features proposed and we achieved a great accuracy of 86.11%. Consequently, the dataset in this paper can be used to classify music produced in the northeast region of Brazil.</p>

			<p>Two important algorithms have been used to feature selection in Music Emotion Retrieval: ReliefF, RReliefF and PCA. As a future work, a feature selection on the set of proposed features may be done to evaluate what are the most relevant features to MGR, excluding the features which presented low importance.</p>

			<p>In this paper we used new features that is not present in other papers analyzed by us. Since the scope of this paper is not about individual features itself, a feature work is needed. The task will be to analyze each new feature as presented by Holzapfel et al. [2] and check the importance of these features to MGR.</p>

			<p>For a better accuracy of the brazilian music dataset, it's important to select more musics to compose our dataset and create a bigger dataset.</p>

			<p>We built our dataset using rules presented by Sturm [3]. Some of these rules are: use a balanced number of samples by each genre, use the best part of the music which represent the genre of the song, use the same length of music for all songs in the dataset and do not repeat the same song. Moreover, our dataset contains 3 important genres to brazilian music that is not found in any other shared dataset. The genres are: Repente, Brega and MPB. Thus, a future task may be to add the BMD to another dataset in order to have a dataset which represent a wide variety of genres.</p>
		</section>
	</article>




